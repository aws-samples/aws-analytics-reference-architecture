{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"What is the Analytics Reference Architecture","text":"<p>Note</p> <p>this project is deprecated in favor of the AWS Data Solutions Framework. AWS DSF provides not only examples but also components that can be directly reused by AWS partners and customers. Popular constructs from this project are being migrated step by step into AWS DSF.</p> <p>The AWS Analytics Reference Architecture is a set of analytics solutions put together as end-to-end examples. It regroups AWS best practices for designing, implementing, and operating analytics platforms through different purpose-built patterns, handling common requirements, and solving customers' challenges.</p> <p>This project is composed of:</p> <ul> <li> <p>Reusable core components exposed in an AWS CDK (Cloud Development Kit) library currently available in Typescript and Python. This library contains AWS CDK constructs that can be used to quickly provision analytics solutions in demos, prototypes, proof of concepts and end-to-end reference architectures. </p> </li> <li> <p>Reference architectures consumming the reusable components to demonstrate end-to-end examples in a business context. Currently, the AWS native reference architecture is available.</p> </li> </ul> <p>This documentation presents the AWS native reference architecture. It explains the journey of a fake company, MyStore Inc., into implementing its data platform solution with AWS products and services.</p>"},{"location":"#business-story","title":"Business story","text":"<p>MyStore Inc. is a US based retailer that operates in multi-channels with an e-commerce platform and physical stores across the United States.</p> <p>The company\u2019s e-commerce platform has recently been implemented with cloud native solutions and purpose built databases. Website sales and customers data are well documented and available in real time. Physical stores are all still operating with on premise solutions based on legacy technologies inherited from various acquisitions. Data is available in batch and in different formats</p>"},{"location":"#mystores-project","title":"MyStore's project","text":"<p>MyStore is building an analytics platform on top of AWS to answer various identified business cases, following the AWS well architected pillars:</p> <ul> <li>Security</li> <li>Operational excellence</li> <li>Cost optimization</li> <li>Performance efficiency</li> <li>Reliability</li> </ul> <p>MyStore is looking for solutions which will remove the manual processes and provide the business with a consistent view of sales and customers across the different channels (web, catalogs and stores)</p> <p>MyStore also wants to improve its analytics platform in the future with AI/ML predictions and recommendations like product recommendation for online customers/mailing for all customers, sales forecasts per channel/region/store, demand forecasts per store</p>"},{"location":"#content","title":"Content","text":"<p>MyStore provides both code and documentation about its analytics platform:</p> <ul> <li>Documentation is available on this website and decomposed into two different parts:<ul> <li>The high level design describes the overall data platform implemented by MyStore, and the different components involved. This is the recommended entry point to discover the solution</li> <li>The analytics solutions provide fine-grained solutions to the challenges MyStore met during the project. These technical patterns can help you choose the right solution for common challenges in analytics area</li> </ul> </li> <li>Code is publicly available here and can be reused as an example for other analytics platform implementations (it should not be reused as-is in production). The code can be deployed in an AWS account following the getting started guide</li> </ul>"},{"location":"high-level-design/architecture/","title":"Architecture","text":""},{"location":"high-level-design/architecture/#architecture","title":"Architecture","text":"<p>MyStore\u2019s analytics platform is implemented in purpose built modules. They are decoupled and can be independently provisioned but still integrate with each others in the global platform. Currently available modules are:</p> <ul> <li>Data Lake foundations: this mandatory module is the core of the analytics platform. It contains the data lake storage and associated metadata for both batch and streaming data.</li> <li>Batch analytics: this module is in charge of ingesting and processing data from Stores channel generated by the legacy systems in batch mode. Data is then exposed to other modules for downstream consumption.</li> <li>Streaming analytics: this module is ingesting and processing real time data from the Web channel generated by cloud native systems. The solution minimizes data analysis latency but also to feed the data lake for downstream consumption.</li> <li>Data Warehouse: this module is ingesting data from the data lake to support reporting, dashboarding and adhoc querying capabilities. The module is using ELT to transform the data from the Data Lake foundations module.</li> <li>Data Visualization: this module is providing dashboarding capabilities to business users like data analysts on top of the Data Warehouse module, but also data exploration on top of Data Lake module.</li> </ul> <p></p>"},{"location":"high-level-design/data-sources/","title":"Data Sources","text":"<p>MyStore's analytics platform is consuming various data sources from its operational systems and is planning to extend its solution by adding more data based on new business cases requirements. Data is coming from two different channels, website and stores. MyStore is building a unique analytics platform to handle both of them.</p>"},{"location":"high-level-design/data-sources/#data-sources","title":"Data sources","text":"<p>Here is the current list of data sources:</p> <ul> <li>Web sale: sales coming from the Web channel. It contains one row per item that can be grouped by order ID</li> <li>Store sale: sales coming from the Store channel. It contains 2 different formats coming from different operational systems:</li> <li>One row per item that can be grouped by ticket ID</li> <li>One JSON object per sale (identified by a ticket ID) with all the items of the sale in nested array. This data is not yet consumed by the analytics platform.</li> <li>Web customer: customers from the Web channel identified by their unique ID. When customers update their personal information, they keep the same ID</li> <li>Web customer address: customer addresses coming from the Web channel including Customer demographics and Household demographics. They are identified by a Customer Address ID. Each update in the address of customers generate a new ID</li> <li>Store customer: customers from the Store channel identified by their unique ID. When customers update their personal information, they keep the same ID</li> <li>Store customer address: customer addresses coming from the Store channel including Customer demographics and Household demographics. They are identified by a Customer Address ID. Each update in the address of customers generate a new ID</li> <li>Promotion: promotions available on items and identified by a promo ID</li> <li>Item: items available for purchase and identified by item ID</li> <li>Warehouse: warehouses responsible for delivering items from Web sales and identified by warehouse ID</li> <li>Store: stores where customers purchased items (grouped in sales) and identified by store ID</li> </ul>"},{"location":"high-level-design/data-sources/#data-model","title":"Data model","text":""},{"location":"high-level-design/modules/batch/","title":"Batch Analytics","text":""},{"location":"high-level-design/modules/batch/#data-pipeline","title":"Data pipeline","text":"<p>The periodically ingested batch data, stored in the raw data S3 bucket, has to be cleansed and normalized on its format,  before it gets stored in the clean data S3 bucket to be processed by any downstream data lake consumers.</p> <p>The data preparation process leverages various features of AWS Glue,  a serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development via Apache Spark framework. The orchestration of the preparation is handled using AWS Glue Workflows  that allows managing and monitoring executions of ETL activities involving multiple crawlers, jobs, and triggers.  The metadata management is implemented via AWS Glue Crawlers,  a serverless process that crawls data sources and sinks to extract the metadata including schemas, statistics and partitions, and saves them in the AWS Glue Data Catalog.</p> <p>The following steps outline the activities involved in the preparation process, which gets triggered on a regular interval,  e.g. every 30 minutes:</p> <p></p> <p>The AWS Glue Workflow maintains strict dependencies between each level of the execution. Only if all activities on a certain level finished successfully,  the activities of the next level get started.</p> <ol> <li>An AWS Glue Crawler is triggered to scan the raw data S3 bucket for newly arrived data entities, partitions, and schemas. The crawler stores and updates the extracted metadata in the raw database in the AWS Glue Data Catalog.</li> <li>The actual cleansing and transformation of the data is implemented as an AWS Glue ETL Script, using PySpark, and executed as an AWS Glue Job. Each entity, like customer, customer address, sale, etc., is processed by an independent job. Each job reads its entity's raw data from the raw data S3 bucket, leveraging the metadata stored in the raw database of the AWS Glue Data Catalog. It then applies cleansing and data type, as well as data format transformations. Finally, it stores the result partitioned by event date in Parquet format into the clean data S3 bucket.</li> <li>An AWS Glue Crawler is triggered to scan the clean data S3 bucket for newly arrived data entities, partitions, and schemas. The crawler stores and updates the extracted metadata in the clean database in the AWS Glue Data Catalog. The clean database can be leveraged by the data lake downstream consumers.</li> </ol> <p>A more detailed description of the batch processing challenges and how to solve them can be found in the Batch Analytics Solutions section.</p>"},{"location":"high-level-design/modules/data-lake/","title":"Data Lake","text":""},{"location":"high-level-design/modules/data-lake/#data-lake-storage","title":"Data Lake storage","text":"<p>The data lake storage is a central component to the overall analytics platform. A data lake is the single source of truth for all data consumers,  it's important to integrate all the modules (Batch Analytics, Streaming Analytics and Data Warehouse) to it. For example, the Streaming module reads Raw layer data to enrich its stream of events with static referential data but  also writes back curated data to the data lake after the processing for cold analytics from the data lake.</p> <p>The data lake storage is implemented with Amazon S3, a durable,  scalable and cost-effective object storage that is fully integrated with analytics tools.</p> <p>The following storage architecture explain how the data lake storage is designed.</p> <p></p> <p>The data lake is organized in multiple Amazon S3 Buckets representing different versions of the data:</p> <ul> <li>The raw layer contains the data coming from the data sources in the raw format without any transformation.     In this layer, the schema is very loose to allow the ingestion of new formats minimizing the errors to avoid missing data.</li> <li>The cleaned layer contains the data from raw that has been cleaned and parsed to a consumable schema.     The data model isn't modified, it contains the exact same entities and relations as in the raw data.     It is the first layer that can be consumed by business users. The schemas and data types are more restrictive but still support schema evolution if it's required.</li> <li>The curated layer contains refactored data based on business requirements. The data can be duplicated in different models and schemas based on the query patterns.     A good example is the Data Warehouse which may require the data in a specific multi-dimensional model.     With lake house design, the Data Warehouse could directly access the data lake curated layer.</li> </ul> <p>There are actually 3 different S3 buckets, one for each layer, but it can be extended to multiple buckets in multiple accounts in the future.</p>"},{"location":"high-level-design/modules/data-lake/#data-lake-governance","title":"Data Lake governance","text":"<p>The data lake catalog is another central component to the overall analytics platform.  A single source of truth requires a catalog to share schemas and technical information like statistics and partitions.  It ensures all the data producers and consumers are using the same metadata.</p> <p>The metadata catalog is implemented with AWS Glue Data Catalog, a serverless catalog services that allows to register, find and access data in multiple data stores. </p> <p>All the data lake datasets metadata are maintained in AWS Glue Data Catalog. Similarly to the data lake storage, the catalog is organized in the same three different layers (raw, cleaned, curated) via three AWS Glue Databases.</p>"},{"location":"high-level-design/modules/data-visualization/","title":"Data Visualization","text":""},{"location":"high-level-design/modules/data-visualization/#architecture","title":"Architecture","text":"<p>The data visualization provide a single entry point to analyze the different data sources and extract value via dashboards, reports and adhoc analysis.</p> <p>It is implemented with Amazon Quicksight, a scalable, serverless, embeddable and machine learning-powered business intelligence tool. Amazon QuickSight is connected to the data lake via Amazon Athena and the data lake via Amazon Redshift using direct query mode, in opposition to the caching mode with SPICE.</p> <p></p>"},{"location":"high-level-design/modules/data-visualization/#data-lake","title":"Data Lake","text":"<p>The following steps outline the connection of Amazon QuickSight to the data lake:</p> <ol> <li>An Amazon Athena data source is used in Amazon QuickSight in order to connect to the data in the data lake.</li> <li>An Amazon QuickSight data set is created, with a direct SQL query to the clean data within the data lake - this data set can be used to create self-service analysis of the data.</li> <li>A pre-built dashboard is deployed which provides visualisation of the data lake Clean data. </li> </ol>"},{"location":"high-level-design/modules/data-visualization/#data-warehouse","title":"Data Warehouse","text":"<p>The following steps outline the connection of Amazon QuickSight to the data lake:</p> <ol> <li>As the Amazon Redshift cluster is within a private subnet, a VPC connection is made from Amazon QuickSight to the VPC.</li> <li>An Amazon Redshift data source is used in Amazon QuickSight in order to connect to the data lake, using credentials for a data analyst user.</li> <li>An Amazon QuickSight data set is created, with a direct SQL query to the data lake - this data set can be used to create self-service analysis of the data.</li> <li>A pre-built dashboard is deployed which provides visualisation of the data within the data lake. </li> </ol>"},{"location":"high-level-design/modules/data-warehouse/","title":"Data Warehouse Data Loading","text":""},{"location":"high-level-design/modules/data-warehouse/#data-pipeline","title":"Data Pipeline","text":"<p>The data warehouse implements an ELT strategy to ingest data from the data lake.  The ELT strategy consists of loading prepared data from the Clean layer of the data lake into the data warehouse and then transforming it into a specific data model optimized for the business queries.</p> <p>The following steps outline the data pipeline from the data lake into the data warehouse:</p> <p></p> <ol> <li>AWS Glue Workflow reads CSV files from the Raw layer of the data lake and writes them to the Clean layer as Parquet files.</li> <li>Stored procedures in Amazon Redshift\u2019s <code>stg_mystore</code> schema extract data from the Clean layer of the data lake using  Amazon Redshift Spectrum via the <code>ext_mystore</code> external schema imported from the AWS Glue Data Catalog  for incremental loads, or the COPY command for history loads.</li> <li>The stored procedures then transform and load the data into a star schema model in the <code>dw_mystore</code> schema.</li> </ol>"},{"location":"high-level-design/modules/data-warehouse/#incremental-loads","title":"Incremental Loads","text":"<p>This diagram describes the process flow for the incremental load of the <code>customer_dim</code> dimension table.</p> <p></p> <ol> <li>The incremental load process is initiated by calling the stored procedure <code>sp_run_incremental_load</code>. In production, <code>sp_run_incremental_load</code> is called every 30 mins by the  Amazon Redshift Data API via an  AWS Step Function. No parameters are required for this  procedure. The procedure loads each target star schema table in series, dimensions first then facts. The following steps  explain the loading of the <code>customer_dim</code> dimension.</li> <li><code>sp_run_incremental_load</code> calls the <code>sp_load_customer_dim</code> stored procedure.</li> <li><code>sp_load_customer_dim</code> selects a high watermark timestamp from the <code>dw_incremental_dates</code> table for each of the  customer_dim\u2019s source tables. A high watermark is a timestamp that records the maximum date and time of the data from  the previous load. This is used as a start date for the next load, ensuring only new or changed data is processed.</li> <li><code>sp_load_customer_dim</code> selects data from tables in the <code>ext_mystore</code> external schema, external tables return  data from the data lake via Amazon Redshift Spectrum. This procedure then transforms and loads the data into the <code>wrk_customer_dim</code>  working table. A working table is a normal table used to temporarily hold data to be processed and is cleared out at  the beginning of every load. An Amazon Redshift TEMPORARY  table could be used for this purpose; however, to facilitate debugging, a standard table has been used with the BACKUP NO  setting to prevent the data from being copied to snapshots.</li> <li><code>sp_load_customer_dim</code> performs slowly changing dimension (SCD) type 1 and 2 logic loading into the target dimension  <code>customer_dim</code>. The type of load into the target will be different depending on what kind of dimensional object is being  loaded. SCD type 1 dimensions and fact tables are merged into by  replacing existing rows  in the target. Dimensions with SCD type 2 fields are loaded using multi-step logic. Surrogate keys are assigned to new  dimension records by an IDENTITY column in the  target table.</li> </ol>"},{"location":"high-level-design/modules/data-warehouse/#history-loads","title":"History Loads","text":"<p>This diagram describes the process flow for the history load of the <code>sale_fact</code> table. History loads for other fact tables  use a similar process.</p> <p></p> <ol> <li>The history load process for the sale_fact table is initiated by calling the stored procedure <code>sp_load_sale_fact</code>. This is a manual process run by an administrator via a SQL client tool when needed. The parameters <code>p_load_type VARCHAR</code>, <code>p_start_date DATE</code> and <code>p_end_date DATE</code> are required for this procedure. <code>p_load_type</code> needs to be set to <code>'HISTORY'</code>, <code>p_start_date</code> and <code>p_end_date</code> needs to be set to the date range to be loaded.</li> <li><code>sp_load_sale_fact</code> loops through the given date range and loads the <code>store_sale</code> staging table one day at a time with the COPY command. The procedure then selects data from the staging table, applies transformations, looks up surrogate keys from dimension tables in the <code>dw_mystore</code> schema, and loads the data into the <code>wrk_sale_fact</code> working table. The surrogate key lookups are performed by joining to dimension tables on the source system natural keys and returning the surrogate primary keys from the dimensions.</li> <li><code>sp_load_sale_fact</code> merges data from the working table into the target fact table. The merge is performed by joining  the working table to the fact table, deleting rows out of the fact that exist in the working table, and then inserting  all rows from the working table into the fact. This is all performed inside a transaction, so if any step fails, the  whole change is rolled back. This merge process is explained in depth in the  Redshift Database Developer Guide.</li> </ol>"},{"location":"high-level-design/modules/source-to-target-mapping/","title":"Source to Target Mappings","text":""},{"location":"high-level-design/modules/source-to-target-mapping/#date-dim","title":"Date Dim","text":"<p>Table Name: date_dim</p> <p>Table Type: Dimension</p> <p>Description:    Each row in this table represents one calendar day.</p> <p>Load Frequency: Static (load once)</p> <p>Source: Loaded once when the DW is deployed from a DDL file.</p> Target Schema: Target Table: Target Column: Target Datatype: Key: SCD Type: Transformation: Note: dw_mystore date_dim date_key date PK N/A Using the date datatype for the key as it will allow range restricted scans on fact tables. dw_mystore date_dim day_date date NK N/A Source column is repeated here for user convenience as primary keys are often hidden in BI presentation layers. dw_mystore date_dim month_seq int N/A dw_mystore date_dim week_seq int N/A dw_mystore date_dim quarter_seq int N/A dw_mystore date_dim year int N/A dw_mystore date_dim dow int N/A dw_mystore date_dim moy int N/A dw_mystore date_dim dom int N/A dw_mystore date_dim qoy int N/A dw_mystore date_dim fy_year int N/A dw_mystore date_dim fy_quarter_seq int N/A dw_mystore date_dim fy_week_seq int N/A dw_mystore date_dim day_name varchar(9) N/A dw_mystore date_dim quarter_name varchar(6) N/A dw_mystore date_dim holiday varchar(1) N/A dw_mystore date_dim weekend varchar(1) N/A dw_mystore date_dim following_holiday varchar(1) N/A dw_mystore date_dim first_dom int N/A dw_mystore date_dim last_dom int N/A dw_mystore date_dim same_day_ly int N/A dw_mystore date_dim same_day_lq int N/A dw_mystore date_dim current_day varchar(1) N/A dw_mystore date_dim current_week varchar(1) N/A dw_mystore date_dim current_month varchar(1) N/A dw_mystore date_dim current_quarter varchar(1) N/A dw_mystore date_dim current_year varchar(1) N/A dw_mystore date_dim dw_insert_date timestamp N/A SYSDATE on Insert SYSDATE on Insert dw_mystore date_dim dw_update_date timestamp N/A SYSDATE on Update SYSDATE on Update"},{"location":"high-level-design/modules/source-to-target-mapping/#time-dim","title":"Time Dim","text":"<p>Table Name:     time_dim</p> <p>Description:    Each row in this table represents one second.</p> <p>Load Frequency: Static (load once)</p> <p>Source:         Loaded once when the DW is deployed from a DDL file.</p> Target Schema: Target Table: Target Column: Target Datatype: Key: SCD Type: Transformation: dw_mystore time_dim time_key int PK N/A dw_mystore time_dim hour int NK N/A dw_mystore time_dim minute int NK N/A dw_mystore time_dim second int NK N/A dw_mystore time_dim am_pm varchar(2) N/A dw_mystore time_dim shift varchar(20) N/A dw_mystore time_dim sub_shift varchar(20) N/A dw_mystore time_dim meal_time varchar(20) N/A dw_mystore time_dim dw_insert_date timestamp N/A SYSDATE on Insert dw_mystore time_dim dw_update_date timestamp N/A SYSDATE on Update"},{"location":"high-level-design/modules/source-to-target-mapping/#customer-dim","title":"Customer Dim","text":"<p>Table Name: customer_dim</p> <p>Table Type: Dimension</p> <p>Description:    Each row in this table represents a customer.</p> <p>Load Frequency: Dynamic</p> <p>Source Tables:  store_customer, store_customer_address</p> <p>Source Table Joins: <code>FROM stg_mystore.v_store_customer c LEFT OUTER JOIN stg_mystore.v_store_customer_address a ON c.address_id\u00a0= a.address_id</code></p> <p>Transformation Notes: </p> <p>All the columns from the store_customer table are treated as SCD1 in the target.</p> <p>All the columns from the store_customer_address table are treated as SCD2 in the target.</p> <p>Null source values are transformed to 'Unknown' or '#' in the target, depending on the target column width. Null source dates/timestamps are transformed to\u00a0'01/01/1000'.</p> <p>The source data lake tables are HUDI tables. HUDI returns timestamps in the Unix time (epoch) format. The Unix time columns are converted via a SQL function and are encapsulated in views: v_store_customer and v_store_customer_address.</p> Source Data Lake Layer: Source Database: Source Table: Source Column: Source Datatype: Target Schema: Target Table: Target Column: Target Datatype: Key: SCD Type: Transformation: Note: dw_mystore customer_dim customer_key int PK N/A Create new surrogate key for new records and SCD2 changes. clean ara_clean_data_ store_customer customer_id string dw_mystore customer_dim customer_id varchar(16) NK N/A clean ara_clean_data_ store_customer salutation string dw_mystore customer_dim salutation varchar(10) 1 The full history is updated on SCD1\u00a0columns for a change in the source. clean ara_clean_data_ store_customer first_name string dw_mystore customer_dim first_name varchar(20) 1 clean ara_clean_data_ store_customer last_name string dw_mystore customer_dim last_name varchar(30) 1 clean ara_clean_data_ store_customer birth_country string dw_mystore customer_dim birth_country varchar(20) 1 clean ara_clean_data_ store_customer email_address string dw_mystore customer_dim email_address varchar(50) 1 clean ara_clean_data_ store_customer birth_date date dw_mystore customer_dim birth_date date 1 clean ara_clean_data_ store_customer gender string dw_mystore customer_dim gender varchar(10) 1 if 'M' then 'Male'\u00a0if 'F' then 'Female'\u00a0else gender clean ara_clean_data_ store_customer marital_status string dw_mystore customer_dim marital_status varchar(10) 1 if 'D' then 'Divorced'\u00a0if 'M' then 'Married'\u00a0if 'S' then 'Single'\u00a0if 'U' then 'Unknown'\u00a0if 'W' then 'Widowed'\u00a0else marital_status clean ara_clean_data_ store_customer education_status string dw_mystore customer_dim education_status varchar(10) 1 clean ara_clean_data_ store_customer purchase_estimate bigint dw_mystore customer_dim purchase_estimate int 1 clean ara_clean_data_ store_customer credit_rating string dw_mystore customer_dim credit_rating varchar(10) 1 clean ara_clean_data_ store_customer buy_potential string dw_mystore customer_dim buy_potential varchar(10) 1 clean ara_clean_data_ store_customer vehicle_count bigint dw_mystore customer_dim vehicle_count int 1 clean ara_clean_data_ store_customer lower_bound bigint dw_mystore customer_dim income_band_lower_bound int 1 clean ara_clean_data_ store_customer upper_bound bigint dw_mystore customer_dim income_band_upper_bound int 1 clean ara_clean_data_ store_customer customer_datetime date dw_mystore customer_dim start_date date 1 f_from_unixtime(customer_datetime) clean ara_clean_data_ store_customer_address address_id string dw_mystore customer_dim address_id varchar(16) 2 clean ara_clean_data_ store_customer_address address_datetime string dw_mystore customer_dim address_date timestamp 2 f_from_unixtime(address_datetime) clean ara_clean_data_ store_customer_address street string dw_mystore customer_dim street varchar(20) 2 clean ara_clean_data_ store_customer_address city string dw_mystore customer_dim city varchar(60) 2 clean ara_clean_data_ store_customer_address county string dw_mystore customer_dim county varchar(30) 2 clean ara_clean_data_ store_customer_address state string dw_mystore customer_dim state varchar(2) 2 clean ara_clean_data_ store_customer_address zip string dw_mystore customer_dim zip varchar(10) 2 clean ara_clean_data_ store_customer_address country string dw_mystore customer_dim country varchar(20) 2 clean ara_clean_data_ store_customer_address gmt_offset string dw_mystore customer_dim gmt_offset numeric(5) 2 clean ara_clean_data_ store_customer_address location_type string dw_mystore customer_dim location_type varchar(20) 2 dw_mystore customer_dim scd_start_date timestamp N/A On SCD2 change:- store_customer.customer_datetime on first record.- store_address.address_datetime on the new record dw_mystore customer_dim scd_end_date timestamp N/A On SCD2 change:\u00a0- store_address.address_datetime on the existing current record\u00a0- '31/12/2999' on the new record dw_mystore customer_dim scd_current_flag varchar(1) N/A On SCD2 change:\u00a0- 'N' on existing record\u00a0- 'Y' on new record dw_mystore customer_dim dw_insert_date timestamp N/A SYSDATE on Insert dw_mystore customer_dim dw_update_date timestamp N/A SYSDATE on Update"},{"location":"high-level-design/modules/source-to-target-mapping/#sale-fact","title":"Sale Fact","text":"<p>Table Name: sale_fact</p> <p>Table Type: Fact</p> <p>Description:    Each row in this table represents a single line item from a sale.</p> <p>Load Frequency: Dynamic</p> <p>Source Tables:</p> <p>Lake: store_sale</p> <p>DW: date_dim, time_dim, customer_dim</p> Source Data Lake Layer: Source Database: Source Table: Source Column: Source Datatype: Target Schema: Target Table: Target Column: Target Datatype: Key: Transformation: Note: N/A Sourced from DW dw_mystore schema date_dim date_key date dw_mystore sale_fact sold_date_key date Surrogate key lookup on:\u00a0SELECT NVL(dd.date_key, TO_DATE('01-JAN-1900', 'DD-MON-YYYY'))\u00a0FROM clean_store_sale ss\u00a0LEFT OUTER JOIN dw_mystore.date_dim dd\u00a0ON TRUNC(ss.sale_datetime) = dd.day_date N/A Sourced from DW dw_mystore schema time_dim time_key int dw_mystore sale_fact sold_time_key int Surrogate key lookup:\u00a0SELECT NVL(td.time_key, -1)\u00a0FROM store_sale ss\u00a0LEFT OUTER JOIN dw_mystore.time_dim td\u00a0 ON EXTRACT(hour from ss.sale_datetime) = td.\"hour\"\u00a0AND EXTRACT(min from ss.sale_datetime) = td.\"minute\"\u00a0AND EXTRACT(sec from ss.sale_datetime) = td.\"second\" clean ara_clean_data_ store_sale sale_datetime timestamp dw_mystore sale_fact sold_date timestamp PK N/A Sourced from DW dw_mystore schema customer_dim customer_key int dw_mystore sale_fact customer_key int Surrogate key lookup:\u00a0SELECT NVL(cd.customer_key, -1)\u00a0FROM store_sale ss\u00a0LEFT OUTER JOIN dw_mystore.customer_dim cd\u00a0ON ss.customer_id = cd.customer_id\u00a0AND ss.sold_datetime &gt;= cd.scd_start_date\u00a0AND ss.sold_datetime &lt; cd.scd_end_date clean ara_clean_data_ store_sale ticket_id bigint dw_mystore sale_fact ticket_id varchar(20) PK clean ara_clean_data_ store_sale item_id bigint dw_mystore sale_fact item_id int PK clean ara_clean_data_ store_sale quantity bigint dw_mystore sale_fact quantity int clean ara_clean_data_ store_sale wholesale_cost decimal(15,2) dw_mystore sale_fact wholesale_cost decimal(15,2) clean ara_clean_data_ store_sale list_price decimal(15,2) dw_mystore sale_fact list_price decimal(15,2) clean ara_clean_data_ store_sale sales_price decimal(15,2) dw_mystore sale_fact sales_price decimal(15,2) clean ara_clean_data_ store_sale ext_discount_amt decimal(15,2) dw_mystore sale_fact ext_discount_amt decimal(15,2) clean ara_clean_data_ store_sale ext_sales_price decimal(15,2) dw_mystore sale_fact ext_sales_price decimal(15,2) clean ara_clean_data_ store_sale ext_wholesale_cost decimal(15,2) dw_mystore sale_fact ext_wholesale_cost decimal(15,2) clean ara_clean_data_ store_sale ext_list_price decimal(15,2) dw_mystore sale_fact ext_list_price decimal(15,2) clean ara_clean_data_ store_sale ext_tax decimal(15,2) dw_mystore sale_fact ext_tax decimal(15,2) clean ara_clean_data_ store_sale coupon_amt decimal(15,2) dw_mystore sale_fact coupon_amt decimal(15,2) clean ara_clean_data_ store_sale net_paid decimal(15,2) dw_mystore sale_fact net_paid decimal(15,2) clean ara_clean_data_ store_sale net_paid_inc_tax decimal(15,2) dw_mystore sale_fact net_paid_inc_tax decimal(15,2) clean ara_clean_data_ store_sale net_profit decimal(15,2) dw_mystore sale_fact net_profit decimal(15,2) customer_dim dw_insert_date timestamp SYSDATE on Insert customer_dim dw_update_date timestamp SYSDATE on Update"},{"location":"high-level-design/modules/star-schema-physical-model/","title":"Star Schema Physical Data Model","text":""},{"location":"high-level-design/modules/star-schema-physical-model/#data-model","title":"Data model","text":""},{"location":"high-level-design/modules/streaming/","title":"Streaming Analytics","text":""},{"location":"high-level-design/modules/streaming/#data-pipeline","title":"Data pipeline","text":"<p>The data stream is ingested in a streaming bus then consumed and processed into a streaming engine before being written to an operational data store.</p> <p>The data is ingested in Amazon Kinesis Data Streams, a scalable and durable real-time streaming bus used as the source for real-time analytics pipelines. The processing of the events is handled by Amazon Kinesis Data Analytics, a serverless Apache Flink service that allows to build powerful real-time processing logic. The operational data store relies on Amazon Elasticsearch Service, a fully managed search service compatible with Elasticsearch APIs, also including a managed Kibana to built real-time dashboarding</p> <p>The following steps outline the different components involved in the streaming analytics platform:</p> <p></p> <ol> <li>Amazon Kinesis Data Stream ingests customers, addresses and sales events in real time in dedicated Data Streams. The number of shards is determined by the volume of data</li> <li>Events are consumed by a single Amazon Kinesis Analytics for Apache Flink application </li> <li>The Apache Flink application is processing the events (see Data Processing)</li> </ol>"},{"location":"high-level-design/modules/streaming/#streamed-data","title":"Streamed data","text":"<p>The streamed data is composed of events on customers, addresses and sales. A new event is streamed for creation or update on these objects.</p> <p>Sales contain references to customers by ID, and customers contain references to addresses by ID.</p>"},{"location":"high-level-design/modules/streaming/#reference-data","title":"Reference data","text":"<p>Reference data is slowly moving and stored in the data lake as one or multiple files. These files can be in CSV (in raw layer) or Parquet formats (in clean layer) for example. In our case, the reference data is Items and Promos.</p>"},{"location":"high-level-design/modules/streaming/#data-processing","title":"Data processing","text":"<p>Apache Flink is used because it provides powerful features such as:</p> <ul> <li>low latency processing</li> <li>watermarking for event-time processing</li> <li>per-operator parallelism for horizontal scaling</li> <li>complex event processing (CEP)</li> <li>able to asynchronously access external systems (REST APIs, databases, ...)</li> <li>snapshots and savepoints</li> <li>exactly once processing</li> </ul> <p>This application is stateful and leverages RocksDB internally to store the various states for data and operators.</p> <p>The application successively joins streams with other streams and reference data to enrich an order and denormalise it.</p> <p>When the denormalised event is generated, it's stored in Amazon S3 Curated bucket and sent to Amazon Elasticsearch Service for indexing and exposition.</p> <p>In the future, the application could asynchronously call a REST API for data enrichment such as geocoding (find latitude and longitude from a postal address).</p> <p></p>"},{"location":"high-level-design/modules/streaming/#data-exposition","title":"Data exposition","text":"<p>Amazon Elasticsearch Service is also responsible for serving real-time visualisation queries of the streamed data via search queries,  and securing the data with fine-grained access control,  with a granularity down to the field level.</p> <p>Included in Amazon Elasticsearch Service, Kibana gives the user the ability to discover, visualize and search the indexed data.</p> <p>As Elasticsearch indexes data in a new index for the first time, it tries to guess the structure of the data. In doing so, it creates a very heavy data structure that can usually be optimised.</p> <p>An optimised index template is therefore being registered once with the index name pattern to be used every time a new index matching the pattern is created.</p>"},{"location":"solutions/costs/","title":"Costs Solutions","text":"<p>You can use Cost Explorer to see the cost of the AWS Analytics Reference Architecture globally or module by module.</p> <p>To help with that, we added two tags <code>project-name</code> and <code>module-name</code>, but you need to activate these tags to be able to use them.</p> <p>Note:  the tags are applied to the cost data after they are enabled and this is not retroactive.</p> <p>To activate the tags, go to the Billing &amp; Cost Management Dashboard:</p> <p></p> <p>Then, name sure <code>module-name</code> and <code>project-name</code> tags are both in the <code>Active</code> status.</p> <p></p> <p>All cost data generated after activation will be tagged by these two when they exist.</p>"},{"location":"solutions/data-lake/","title":"Data Lake Solutions","text":""},{"location":"solutions/data-lake/#choosing-the-right-amazon-s3-buckets-design-for-a-data-lake","title":"Choosing the right Amazon S3 buckets design for a Data Lake","text":""},{"location":"solutions/data-lake/#challengeissue","title":"Challenge/Issue","text":"<p>Designing Amazon S3 storage is an important step in the Data Lake project. There are multiple considerations to understand and changing the design afterward can be challenging because of the data volume. This solution explains the Amazon S3 buckets design (numbers and configurations) used to support to the Data Lake</p>"},{"location":"solutions/data-lake/#solution","title":"Solution","text":"<p>Having multiple buckets provides fine-grained capabilities to configure the bucket's security, data retention and other options. On the other it adds additional resources to maintain into the solution.</p> <p>Here is the list of features which are defined at the bucket level:</p> <ul> <li>Bucket policy</li> <li>Lifecycle configuration</li> <li>Requester pays</li> </ul> <p>MyStore did a tradeoff between these two considerations and chose to implement its Data Lake with a multi-layer buckets approach. The Data Lake is composed of three different buckets, one for each state of the data (raw, cleaned, curated). Currently, all the data from the same layer are sharing the same security, retention and costs requirements. The solution is extensible if MyStore wants to apply different settings to a subpart of the tables. New buckets can be added, for example, for each level of security.</p>"},{"location":"solutions/data-preparation/","title":"Batch Analytics Solutions","text":""},{"location":"solutions/data-preparation/#keeping-the-data-catalog-up-to-date","title":"Keeping the Data Catalog Up-To-Date","text":""},{"location":"solutions/data-preparation/#challengeissue","title":"Challenge/Issue","text":"<p>To create a data lake, you must catalog the data stored within the lake. As described in the data lake section of the high-level design, MyStore utilizes the Glue Data Catalog for keeping track of the location, schema, and statistics of the data. There are different ways to populate the metadata in the Data Catalog. A crawler can be used to take inventory of the data in your data stores, but there are various methods to define and update objects manually. Factors like latency in visibility of newly arrived data, dependency management, and cost have to be considered when looking for the right approach to keep your Data Catalog up-to-date. </p>"},{"location":"solutions/data-preparation/#solution","title":"Solution","text":"<p>MyStore new raw data arrives periodically every 30 minutes in the raw S3 bucket. The only process consuming from the raw S3 bucket is the data preparation process, which relies on up-to-date Data Catalog information about all entity types stored in that data lake location. For that reason, the Data Catalog gets updated as the first step in the workflow orchestrating the data preparation process. A single crawler is used and executed for all the entities to avoid multiple code changes in case of newly added entity types, as well as to lower the cost. The crawler is configured to identify any new and all changes to existing entity types and update the Data Catalog accordingly.</p> <p>After the processing of the data (one job per entity), each job updates the Data Catalog to keep the metadata related to the clean S3 bucket up-to-date. The following extract of the data preparation ETL script demonstrate how to create tables, update schemas, and add new partitions in the Data Catalog from AWS Glue ETL Jobs:  <pre><code>sink = glue_context.getSink(connection_type=\"s3\", path=\"s3://\" + output_bucket_name + \"/\" + clean_table_name,\n                                enableUpdateCatalog=True, updateBehavior=\"UPDATE_IN_DATABASE\",\n                                partitionKeys=[partition_key])\nsink.setFormat(\"glueparquet\")\nsink.setCatalogInfo(catalogDatabase=clean_db_name, catalogTableName=clean_table_name)\nsink.writeFrame(DynamicFrame.fromDF(enriched_data, glue_context, 'result'))</code></pre></p> <p>Updating the Data Catalog from within the AWS Glue ETL Job has the benefit that the consumers that rely on the Data Catalog will have immediate access to the newly produced data sets. Furthermore, this approach can lower the cost if no additional crawler has to be executed. To understand the cost of running a crawler, refer to the Glue Pricing page. When evaluating this approach for your use case, please check your requirements against the existing restrictions.</p> <p>In summary, tradeoffs have to be made between:</p> <ul> <li>Latency in data visibility: Strictly separating the cataloging process for each entity type vs. introducing artificial dependencies between the independent entity types.</li> <li>Cost: Running no, one, or multiple crawlers to update the Data Catalog.</li> <li>Complexity: Updating the Data Catalog manually vs. only via crawlers vs. using a mix of manual updates and crawlers. </li> </ul>"},{"location":"solutions/data-preparation/#orchestrating-a-serverless-batch-analytics-pipeline","title":"Orchestrating a serverless batch analytics pipeline","text":""},{"location":"solutions/data-preparation/#challengeissue_1","title":"Challenge/Issue","text":"<p>The usage of a workflow management product is required to schedule and monitor the individual steps of batch processing pipelines, and to manage dependencies between steps.  AWS provides different workflow management tools including AWS Step Functions,  Amazon Managed Workflows for Apache Airflow and AWS Glue Workflows. Also, 3rd party products are available via the AWS Marketplace. Choosing the right workflow management product can be challenging.</p>"},{"location":"solutions/data-preparation/#solution_1","title":"Solution","text":"<p>Looking at the various different workflow management products, you can easily categorize them into general purpose solutions and platform/service-specific solutions. While AWS Step Functions and Amazon Managed Workflows for Apache Airflow can be used for a wide variety of use cases and applications, AWS Glue Workflow is targeted to ease the definition, execution management, and monitoring of workflows consisting of AWS Glue activities, such as crawlers, jobs, and triggers.</p> <p>MyStore's data preparation process is completely decoupled from all up- and down-stream systems via the interfaces of the raw and clean S3 buckets, as well as metadata provided in the Glue Data Catalog, so its workflow management platform choice can be made in isolation from other processes. Furthermore, all individual steps of the data preparation process have been designed and implemented as AWS Glue activities, namely an AWS Glue crawler and, a single parametrised AWS Glue ETL Job for each entity type, as described in the data lake section of the high-level design. Operating solely within the context of AWS Glue, MyStore has chosen to use AWS Glue Workflows, which natively supports all the described requirements that MyStore has for their data preparation process. Leveraging AWS Glue Workflows, all data preparation related processing elements can be found within the AWS Glue console.  </p>"},{"location":"solutions/data-preparation/#creating-a-reusable-etl-script","title":"Creating a reusable ETL Script","text":""},{"location":"solutions/data-preparation/#challengeissue_2","title":"Challenge/Issue","text":"<p>The data preparation process cleanses and transforms a variety of entity types, like the customer, customer address, and sales entities. Each entity type defines its own schema, consisting of different number of fields, field names, and data types. While creating a separate ETL script for each entity type would solve the issue, using a reusable approach that can process any entity type brings benefits on development and maintenance costs.</p>"},{"location":"solutions/data-preparation/#solution_2","title":"Solution","text":"<p>MyStore is using a single parameterized Glue ETL script for each type of data storage format. Currently, MyStore is relying on a combination of Apache Parquet files for event logs data and Apache Hudi for updatable data.  (see the reasons why MyStore is using this solution here). In consequence, the Batch Analytics pipeline is limited to two ETL scripts. Any entity-type-specific source or target information gets defined within the parameters, whose default values are specified in the entity-type-related AWS Glue Job definition. It is important to note that the script does not require to define the entity-type-specific schema.  The script defines and leverages the following parameters:</p> <pre><code>args = getResolvedOptions(sys.argv,\n                          ['JOB_NAME', 'raw_db_name', 'clean_db_name', 'source_entity_name', 'target_entity_name', 'datetime_partition_column',\n                           'partition_key', 'output_bucket_name'])\njob_name = args['JOB_NAME']\nraw_db_name = args['raw_db_name']\nclean_db_name = args['clean_db_name']\nsource_entity_name = args['source_entity_name']\ntarget_entity_name = args['target_entity_name']\ndatetime_partition_column = args['datetime_partition_column']\npartition_key = args['partition_key']\noutput_bucket_name = args['output_bucket_name']\n\nraw_table_name = source_entity_name\nclean_table_name = target_entity_name</code></pre> <p>To read the data from the raw S3 bucket, the variables <code>raw_db_name</code> and <code>raw_table_name</code> are used to refer to the entity-type-specific raw data table in the AWS Glue Data Catalog. Using AWS Glue's DynamicFrameReader 's method to read data from the specified table name, there is no need to specify any reader schema within the script. <pre><code>raw_data: DynamicFrame = glue_context.create_dynamic_frame.from_catalog(database=raw_db_name, table_name=raw_table_name, transformation_ctx=\"raw_data\")</code></pre></p> <p>After reading the data, the script transforms and enriches the raw data. For that, it first gets converted into a DataFrame. <pre><code>input_data = raw_data.toDF()</code></pre></p> <p>Each source, independent of the entity type, carries the <code>processing_datetime</code> in form of a unix timestamp from the ingestion system along. To ease the consumption of the data, the data type is transformed into a timestamp. <pre><code>cleaned_data = input_data.select(*[from_unixtime(c).alias(c) if c == 'processing_datetime' else col(c) for c in input_data.columns])</code></pre></p> <p>As time-related information in the raw is stored as String to allow any ingestion to succeed, the data preparation script transforms these fields accordingly. By convention, any field ending with <code>_datetime</code> gets transformed into a timestamp, and any field ending with <code>_date</code> gets transformed into a date. <pre><code>cleaned_data = cleaned_data.select(*[to_timestamp(c).alias(c) if c.endswith('_datetime') else col(c) for c in input_data.columns])\ncleaned_data = cleaned_data.select(*[to_date(c).alias(c) if c.endswith('_date') else col(c) for c in input_data.columns])</code></pre></p> <p>For a few exceptional cases, the automatic data type detection and conversion gets to its limits. As <code>zip</code> codes are detected as numeric numbers, an explicit cast into String has to be performed. <pre><code>cleaned_data = cleaned_data.select(*[col(c).cast('string').alias(c) if c == 'zip' else col(c) for c in input_data.columns])</code></pre></p> <p>Before being able to write the transformed raw data to the target location, two additional fields have to be added. To be able to track down a record has been touched by each process in the data pipeline, the script adds an <code>etl_processing_datetime</code> fields, which carries the timestamp of when the ETL script has been started. The second fields to be added is required, as the output of the data preparation process in the clean S3 bucket should be partitioned by event time's date. As the source field of the event time, as well as the partition key, is entity-type-specific, the value of the <code>datetime_partition_column</code> parameter determines the name of the source field, and the value of the 'partition_key' parameter defines the name of the newly created partition key column. <pre><code>enriched_data = cleaned_data.withColumn('etl_processing_datetime', unix_timestamp(f.lit(processing_start_datetime), 'yyyy-MM-dd HH:mm:ss').cast(\"timestamp\")).withColumn(partition_key, f.date_format(f.col(datetime_partition_column), \"yyyy-MM-dd\").cast(\"date\"))</code></pre></p> <p>Finally, the script stores the enriched data sets partitioned by the <code>partition_key</code> column in the clean S3 bucket, which is defined in the <code>output_bucket_name</code> parameter. The <code>clean_table_name</code> parameter is used to prefix the object paths of the output and update the metadata in the AWS Glue Data Catalog (in combination with <code>clean_db_name</code> parameter) as described in Keeping the Data Catalog Up-To-Date. <pre><code>sink = glue_context.getSink(connection_type=\"s3\", path=\"s3://\" + output_bucket_name + \"/\" + clean_table_name,\n                            enableUpdateCatalog=True, updateBehavior=\"UPDATE_IN_DATABASE\",\n                            partitionKeys=[partition_key])\nsink.setFormat(\"glueparquet\")\nsink.setCatalogInfo(catalogDatabase=clean_db_name, catalogTableName=clean_table_name)\nsink.writeFrame(DynamicFrame.fromDF(enriched_data, glue_context, 'result'))</code></pre></p> <p>With the provided approach of implementing the ETL script, MyStore ends up in just having a single version of the ETL script stored in S3 and all AWS Glue Jobs referencing to it. The entity-type-specific configuration is defined within the AWS Glue Jobs. However, as seen with the example of the zip code, this approach still requires non-generic code fragments to be placed within the reusable script. Given the simplicity of the variation in the source data schema and formats, this approach is working well for the given scenario of MyStore.</p>"},{"location":"solutions/data-preparation/#handling-of-late-arriving-events","title":"Handling of late arriving events","text":""},{"location":"solutions/data-preparation/#challengeissue_3","title":"Challenge/Issue","text":"<p>Data from the source systems can arrive with delays in the raw S3 bucket. At the same time, data consumers should not miss any data, even when arriving late.</p>"},{"location":"solutions/data-preparation/#solution_3","title":"Solution","text":"<p>The data preparation process consumes its input data from the raw S3 bucket, where the data is partitioned by the processing time of the source system. By aligning the data pipeline end-to-end on the continuously increasing source system's processing time, all late arriving data would automatically be picked up at any stage of the pipeline.</p> <p>However, MyStore decided to provide a more user-friendly view on the data in the clean S3 bucket, which is the output location of the data preparation process, by partitioning the data by event time. Event time being the point in time when the event has been created at its original source. Querying on event time is the more natural way for users to interact with the data than considering a time when a particular system has touched the event.  As event time and processing time can diverge in certain situations, there is no direct correlation between the partition of the input and output data. This design results in updates being made to existing partitions in the clean S3 bucket.</p> <p>While switching partitioning from processing time to event time eases the situation for downstream consumers that run isolated queries on append-only data sets, it creates additional challenges for consumers that are interested in the incremental changes that are applied to a data set. To handle this challenge, consumers that utilize AWS Glue can leverage a feature called job bookmarks. Using this feature, AWS Glue tracks data that has already been processed during a previous run of an ETL job and prevents reprocessing of old data. In case of consuming data from Amazon S3, the tracking happens on object level across all partition. Another option for consumers to identify and consume the incremental changes to the data set is to leverage Amazon S3 event notifications. Using this feature, consumers will be notified when new objects are created in any of the partitions and can process them according to their needs.</p> <p>Choosing an approach to handle late arriving events always has to consider the full data processing pipeline from the data source down to the final consumer of the event. In a flow, which only includes processing steps that are interested in the incremental changes to the data set, sticking to a processing time based partitioning is usually the simplest approach. If, like in the MyStore Inc scenario, a single view should be provided that allows end users to query based on event time, but also processes to pick up incremental changes, additional mechanisms have to be leveraged as described above.</p>"},{"location":"solutions/data-visualization/","title":"Data Visualization Solutions","text":""},{"location":"solutions/data-visualization/#direct-query-vs-caching-with-spice","title":"Direct Query vs. Caching with SPICE","text":""},{"location":"solutions/data-visualization/#challengeissue","title":"Challenge/Issue","text":"<p>How do I know when to use SPICE or direct query for my data set?</p>"},{"location":"solutions/data-visualization/#solution","title":"Solution","text":"<p>When creating an Amazon QuickSight data set you have the option to directly query the underlying data source, such as Amazon Athena, or load the data into Amazon QuickSight\u2019s Super-fast, In-memory Calculation Engine (SPICE). Using SPICE can save time and money </p> <ol> <li>As the further calculations and queries needed by data and business analysts are processing locally instead of waiting for results from direct query on the external data source.</li> <li>As it reduces the overhead on the data lake or data warehouse by allowing the data stored in SPICE to be reused multiple times. </li> </ol> <p>At the time of writing the following row and volume limits apply to the use of SPICE for a dataset:</p> <ul> <li>For Standard edition, 25 million (25,000,000) rows or 25 GB for each dataset</li> <li>For Enterprise edition, 250 million (250,000,000) rows or 500 GB for each dataset</li> </ul> <p>Within the Data Visualisation module direct query is used for both data sources so that as new streaming data arrives it is available immediately for visualisation within Amazon QuickSight. </p> <p>The decision to use SPICE will depend up on a couple of primary factors:</p> <ol> <li>The volume of data required for visualisation</li> <li>The rate at which the data needs to be refreshed</li> </ol>"},{"location":"solutions/data-visualization/#data-volume","title":"Data Volume","text":"<p>If the volume of data required for analysis is larger than the limit listed above then a direct query will be required in order to visualise the data. In this scenario, one pattern that can be followed is to create two datasets in Amazon QuickSight:</p> <ol> <li>A pre-aggregate dataset, that is below the SPICE limit, to provide summary visualisations.</li> <li>An un-aggregated dataset, that is configured for direct query, the summary data can then be used to drill down to a filtered subset of the un-aggregated data. <p>Note: To eliminate the direct query from being made to the full un-aggregated dataset, it is recommended to keep the visual representation of detailed un-aggregated dataset i.e. the <code>detailed visualization</code> in a separate sheet from the visual representation of the pre-aggregated dataset i.e. the <code>summary visualization</code>.</p> <p>Hence, to use Navigation Action to keep passing filters from the <code>summary visualization</code> to the <code>detailed visualization</code>.</p> </li> </ol>"},{"location":"solutions/data-visualization/#data-refresh-rate","title":"Data Refresh Rate","text":"<p>If the data needs to be refreshed more than approximately once per hour then you will need to direct query the underlying data source. </p> <p>SPICE Refresh</p>"},{"location":"solutions/data-visualization/#challengeissue_1","title":"Challenge/Issue","text":"<p>How can I best optimise my use of SPICE, and the underlying data sources?</p>"},{"location":"solutions/data-visualization/#solution_1","title":"Solution","text":"<p>SPICE data can be refreshed manually, via the Amazon QuickSight APIs or on a schedule, for example daily, weekly or monthly. Where possible the recommended best practice is to refresh SPICE only when necessary, this ensures the underlying data sources are used efficiently by only querying the data when an update is required. This can be achieved using an event driven approach, with an example shown in this blog post . </p>"},{"location":"solutions/data-visualization/#amazon-redshift-private-connectivity-from-amazon-quicksight","title":"Amazon Redshift Private Connectivity from Amazon QuickSight","text":""},{"location":"solutions/data-visualization/#challengeissue_2","title":"Challenge/Issue","text":"<p>How to connect Amazon QuickSight to a private Amazon Redshift cluster?</p>"},{"location":"solutions/data-visualization/#solution_2","title":"Solution","text":"<p>Best practice is for Amazon Redshift clusters to not be publicly accessible, all access should be tightly controlled. In order to connect Amazon QuickSight to an Amazon Redshift cluster in a private subnet a VPC Connection can be created within Amazon QuickSight to the VPC. Security groups can be used in order to control the network communication. This article provides step-by-step instructions that detail how this can be configured.</p>"},{"location":"solutions/data-visualization/#sharing-an-amazon-quicksight-analysis-or-dashboard-between-accounts","title":"Sharing an Amazon QuickSight Analysis or Dashboard between Accounts","text":""},{"location":"solutions/data-visualization/#challengeissue_3","title":"Challenge/Issue","text":"<p>How to share Amazon QuickSight analysis between AWS accounts?</p>"},{"location":"solutions/data-visualization/#solution_3","title":"Solution","text":"<p>In order to share either an analysis or dashboard between Amazon QuickSight instances in separate AWS Accounts, a template can be created in the source account. This template can then be used to create a new template, analysis or dashboard in the target account. This article provides a step-by-step example of how this can be configured.</p>"},{"location":"solutions/data-warehouse/","title":"Data Warehouse Solutions","text":""},{"location":"solutions/data-warehouse/#orchestrating-stored-procedures-based-data-warehouse-elt","title":"Orchestrating stored procedures based data warehouse ELT","text":""},{"location":"solutions/data-warehouse/#challengeissue","title":"Challenge/Issue","text":"<p>There are multiple solutions to trigger SQL queries and stored procedures in Amazon Redshift, but some comes with challenges including timeouts and error handling. This solution demonstrate how to orchestrate ELT jobs in the data warehouse from a workflow management tool.</p>"},{"location":"solutions/data-warehouse/#solution","title":"Solution","text":"<p>MyStore is using ELT approach to load and then transform data into its Data Warehouse. The ELT scripts (Amazon Redshift stored procedures) are triggered by an AWS Step Function state machine via Amazon Redshift Data API. The Data API is asynchronous and has many advantages compared to other approaches:</p> <ul> <li>The solution is cheap. A state is an AWS Lambda function invocation that submit a statement and then shutdown. There isn't any resources running while waiting for the ELT to finish.</li> <li>There isn't any risk of timeout like with a synchronous call. AWS Lambda function have a timeout of 15 minutes, if the statement is taking longer to proceed, the workflow may lose the statement execution status.</li> <li>Network errors are fully handled by the state machine which regularly poll for the statement status. With synchronous calls, network communication errors with the client cancels the statement.</li> </ul> <p>Here is the graphical representation of the state machine </p> <ol> <li>The Submit state is an AWS Lambda function that calls the Amazon Redshift Data API execute_statement command</li> <li>The Wait state is waiting for 30s before polling for statement status</li> <li>The Status state is an AWS Lambda function that call the Amazon Redshift Data API describe_statement command</li> <li>The Complete state is checking if the status is FINISHED. If not it loops back into the Wait and Status states. If the statement status is FAILED, the state machine goes into ERROR state. If it's FINISHED, it goes to DwhLoaderSuccess</li> </ol> <p>For security reasons, Amazon Redshift Data API are session scoped so only submitters can see their statements. Currently, the session is based on the AWS Lambda function name, so the AWS Step Function state machine is using the same AWS Lambda function for both submitting statement and polling the statement status.</p> <p>Here is the code for the AWS Lambda function</p>"},{"location":"solutions/data-warehouse/#optimizing-load-performance-for-slowly-changing-dimensions-in-the-data-warehouse","title":"Optimizing load performance for Slowly Changing Dimensions in the Data Warehouse","text":""},{"location":"solutions/data-warehouse/#challengeissue_1","title":"Challenge/Issue","text":"<p>A common requirement of the Data Warehouse is to build Slowly Changing Dimensions to track history and last version of data. For example, to be able to match sales fact records to store_customer dimension records it requires scanning historical data in store_customer table to find the latest record. Each update of the customer is generating a new record in store_customer table. With a time many historical records are accumulated, as a result a lookup query performance will degrade. Bottom line the described approach is not scalable.</p>"},{"location":"solutions/data-warehouse/#solution_1","title":"Solution","text":"<p>To be able to always query the latest state of the record and keep the amount of historical records limited, MyStore introduced the usage of Apache Hudi. AWS Glue clean job is writing directly to Amazon S3 in Hudi format. To have more details about the write implementation and Hudi configuration please look at raw2clean_hudi.py.</p> <p>Hudi format is not supported by AWS Glue Crawler, nevertheless Hudi is supporting the synchronisation with Hive metastore out of the box. To prevent AWS Glue Crawler failure MyStore configured store_customer and store_customer_address tables as exclusions crawler.py. To enable Hive metastore sync, the AWS Glue job is using the Hudi configuration hoodie.datasource.hive_sync in raw2clean_hudi.py. You can find more details about Hudi configuration on this page.</p> <p>Hudi is managing the number of historical records automatically and limited according to the configuration parameter hoodie.cleaner.commits.retained, which is configured to 10.</p> <p>Using Apache Hudi MyStore can guarantee that Amazon Redshift Spectrum lookup query performance is not affected by the number of historical updates in store_customer and store_customer_address tables and will always get the lately updated record.</p>"},{"location":"solutions/streaming/","title":"Streaming Solutions","text":""},{"location":"solutions/streaming/#exactly-once-stream-processing","title":"Exactly once stream processing","text":""},{"location":"solutions/streaming/#challengeissue","title":"Challenge/Issue","text":"<p>How do I make sure I have exactly once processing / delivery?</p>"},{"location":"solutions/streaming/#solution","title":"Solution","text":"<p>Exactly once processing ensures that one streamed message is processed only once by the system, while exactly once delivery ensures that the result of processing appears only once in the destination systems.</p> <p>Although Amazon Kinesis Data Streams itself only provides at-least-once, the Amazon Kinesis Flink connector supports exactly-once.</p> <p>See Apache Flink documentation for reference.</p> <p>Notice that the Elasticsearch connector only provides at-least-once guarantee. In the case of Elasticsearch, the solution is to provide a consistent document id so that a second copy will overwrite the first one instead of adding an additional entry.</p>"},{"location":"solutions/streaming/#use-avro-logical-types-with-amazon-kinesis-data-analytics","title":"Use Avro logical types with Amazon Kinesis Data Analytics","text":""},{"location":"solutions/streaming/#challengeissue_1","title":"Challenge/Issue","text":"<p>How to use Avro schemas with logical types in Amazon Kinesis Data Analytics.</p>"},{"location":"solutions/streaming/#solution_1","title":"Solution","text":"<p>There is a problem in the serialization support in Flink that has been corrected in Flink 1.11.3:  <p>The solution is to use Flink version 1.11.3 instead of 1.11.1 when compiling and running it against the Amazon Kinesis Data Analytics environment.</p>"}]}